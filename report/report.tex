\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{url,hyperref}
\usepackage{float}
\usepackage[bottom]{footmisc}
\usepackage{fancyvrb}
\usepackage{textcomp}
\hypersetup{
	colorlinks = true,
	citecolor = green,
	urlcolor = cyan
}

\title{
	Web Information Extraction and Retrieval\\
	Programming Assignment 2: \\
	Crawler implementation
}
\author{
	Marko Prelevikj\\
	63130345\\
	\texttt{mp2638@student.uni-lj.si}
	\and
	Gojko Hajduković\\
	63180431\\
	\texttt{gh8590@student.uni-lj.si}
	\and
	Stefan Ivanišević\\
	63170405\\
	\texttt{si0539@student.uni-lj.si}
}
\date{May 2019}

\begin{document}
	
	\maketitle
	
	\section{Introduction}
	Nowadays, \texttt{World Wide Web} represents an inexhaustible source of information with its size constantly and rapidly enlarging from day to day. Most of the pages that the \texttt{WWW} consists of are pages created according to some predefined, structured layout. Extraction of data from the Web pages is very useful and widely used today in many applications for all sorts of purposes from web scraping, web and data mining to all sorts of monitoring such as weather data monitoring, real-estate listing, price comparisons, etc. In this paper we introduce three different approaches in structured data extraction (\textit{Regular expressions, Xpath queries} and \textit{RoadRunner}) from three different types of web pages(\textit{\href{https://www.overstock.com/}{Overstock.com}, \href{https://www.rtvslo.si/}{RtvSlo.si}} and \textit{\href{http://www.autodiler.me}{Autodiler.me}}). Explanation and implementation specifics are provided in following sections.\footnotemark
	\footnotetext{Data is available on our GitHub \href{https://github.com/pr3mar/ieps-parser}{repository}, alongside with source code and results from our implementations.}
	
	\section{Chosen web page}
	For our web site of choice, we have picked the \href{http://www.autodiler.me}{audodiler.me} web page. From this web site, we have a page representing a  \href{http://www.autodiler.me/auto_oglasi_auto}{car} query and another one for \href{http://www.autodiler.me/auto_oglasi_kamion}{trucks}. The pages are  downloaded and made available for offline reading i.e. extracting within the \textit{inputs} folder. Data (e.g. "Name", "Date", "Year made", etc.) that we are going to extract from the above-mentioned pages is presented in Figure~\ref{fig:adiler}.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=8cm]{adiler.png}
		\caption{AutoDiler.me Web page sample}
		\label{fig:adiler}
	\end{figure}
	
	\section{Regular Expression}
	A regular expression is a string of text that allows us to create patterns that help match, search, and manipulate text. Regex is very helpful when having to parse a large amount of text. In our example, for extracting predefined data from the HTML pages we have used a regex that are defined bellow.
	\subsection{Overstock.com}
	\texttt{Overstock} represent a "listing" type of Web page, having many data records that have the same layout. The regular expression used for extracting data items for a data record is:    
	\begin{Verbatim}
	[\d\-kKt]{5}.*?)<\/b>|
	<s>(.*?)<\/s>|
	<span class=\"bigred\"><b>(\$\d*,*\d+.\d+)</b></span>|
	<span class=\"littleorange\">(\$\d*,*\d+.\d+)*\s\((\d+\%)\)</span>|
	<span class=\"normal\">\s*(.*?)\s*<br>
	\end{Verbatim}
	Extraction of data is done using a single regular expression using multiple logical \textit{OR}s. We did this to illustrate the power of regular expressions. This approach of extracting data is much shorter but it's not as readable as the next two approaches we used. 
	
	\subsection{RtvSlo.si}
	Regular expressions used for extracting data items from the \texttt{RtvSlo} web pages are:
	\begin{itemize}
		\item \textbf{Author} - \verb|<div class=\"author-name\">(.*?)<\/div>|
		\item \textbf{Published Time} - \verb|<div class=\"publish-meta\">[\n\s]*(.*?)<br>|
		\item \textbf{Title} - \verb|<h1>(.*)<\/h1>| 
		\item \textbf{Subtitle} - \verb|<div class=\"subtitle\">(.*?)<\/div>|
		\item \textbf{Lead} - \verb|<p class=\"lead\">(.*?)<\/p>|
		\item \textbf{Content} - \verb|<article.*?<p.*?>(.+)<\/p>.*<\/article>|
	\end{itemize}
	
	\subsection{AutoDiler.me}
	Regular expressions used for extracting data items for a data record from a \texttt{Autodiler} web pages shown in Figure~\ref{fig:adiler} are:
	\begin{itemize}
		\item \textbf{Name} - \verb|<h1>.*?>(.*?)<|
		\item \textbf{Image} - \verb|<div class=\"oglas_thumb.*?\">.*?<img src=\"(.*?.jpg)\"|
		\item \textbf{Date} - \verb|Datum: ([\d\.]+)*\.|
		\item \textbf{Year} - \verb|Godi.*?te:</strong> <span>(\d{4})|
		\item \textbf{Kilometers} - \verb|Kilometra.*?a:</strong> <span>(\d+)|
		\item \textbf{Fuel} - \verb|Gorivo:</strong> <span>(\w+)<| 
		\item \textbf{City} - \verb|Grad:</strong> <span>(.*?)<|
		\item \textbf{Current price and Old price} - \verb|<div class=\"priceWrapper\"><span class=\"|
		\verb|currentPrice\">(\d+ \€)(<\/span><span class=\"oldPrice\">(\d+ \€)<\/span|
		\verb|><\/div>)*"|
	\end{itemize}
	
	\section{Xpath}
	As one of the approaches in extracting structured data from Web pages we used XML path language, \texttt{Xpath}. Xpath represents a query/path language that is mainly used for finding any element on the Web page based on traversing a tree representation of XML document. We have converted HTML document to a XML document in order to make it accessible for \texttt{Xpath} usage.
	\textit{Absolute} and \textit{relative} \texttt{Xpath} queries are available and used. Even though, absolute \texttt{Xpath} queries are faster, since it is a direct path from the root of a tree to an element,adding or removing an element in the tree makes \texttt{Xpath} query fail. In this project we have used relative \texttt{Xpath} queries.
	
	\subsection{Overstock.com}
	\texttt{Overstock} represent a "listing" type of Web page, having many data records that have a same layout. \texttt{Xpath} queries used for extracting data items for a data record are:
	\begin{itemize}
		\item \textbf{Title} - \texttt{'//a/b[contains(text(),"-kt") or contains(text(),"-Kt") ]/text()'} 
		\item \textbf{List price} - \texttt{'//td[b[contains(text(),"List Price:")]]/following-sibling::td//text()'} 
		\item \textbf{Price} - \texttt{'//td[b[text()="Price:"]]/following-sibling::td//text()'} 
		\item \textbf{Saving(percent)} - \texttt{'//td[b[text()="You Save:"]]/following-sibling::td//text()'} 
		\item \textbf{Content}  -\texttt{'//td[span[@class="normal"]]/span/text()'} 
	\end{itemize}
	
	\subsection{RtvSlo.si}
	Xpath queries used for extracting data items from a \texttt{RtvSlo} web pages are :
	\begin{itemize}
		\item \textbf{Author} - \texttt{'//*[@class="author-name"]//text()'}  
		\item \textbf{Published Time} - \texttt{'//*[@class="publish-meta"]//text()'} 
		\item \textbf{Title} - \texttt{'//*[contains(@class,"news-container")]//header//h1//text()'}  
		\item \textbf{Subtitle} - \texttt{'//*[contains(@class,"news-container")]//header//div[@class="subtitle"]//text()'}  
		\item \textbf{Lead} - \texttt{'//*[contains(@class,"news-container")]//header//p[@class="lead"]//text()'}  
		\item \textbf{Content} - \texttt{'//div[contains(@class,"article-body")]//p//text()'} 
	\end{itemize}
	
	\subsection{AutoDiler.me}
	\texttt{Xpath} queries used for extracting data items for a data record from a \texttt{Autodiler} pages shown in Figure~\ref{fig:adiler} are:
	\begin{itemize}
		\item \textbf{Name} - \texttt{'//*[contains(@class,"oglas\_thumb")]/div[2]//h1//text()'} 
		\item \textbf{Image} - \texttt{'//*[contains(@class,"oglas\_thumb")]/div[1]//img//@src'}
		\item \textbf{Date} - \texttt{'//*[contains(@class,"oglas\_thumb")]/div[3]//span[@class="date"]//text()'} 
		\item \textbf{Year} - \texttt{'//*[contains(text(),"Godište")]/following-sibling::span/text()'} 
		\item \textbf{Kilometres} - \texttt{'//*[contains(text(),"Kilometr")]/following-sibling::span/text()'} 
		\item \textbf{Fuel} - \texttt{'//*[contains(text(),"Gorivo")]/following-sibling::span/text()'}  
		\item \textbf{City} - \texttt{'//*[contains(text(),"Grad")]/following-sibling::span/text()'} 
		\item \textbf{Current price} - \texttt{'//div[contains(@class,"priceWrapper")]/span[1]/text()'} 
		\item \textbf{Old price} - \texttt{'//div[contains(@class,"priceWrapper")]/span[2]/text()'} 
	\end{itemize}
	
	\section{Road Runner}
	A more automated way of extracting data is using more advanced method, such as \textit{RoadRunner}~\cite{crescenzi2001roadrunner}. The purpose of this method is to generate a \textit{wrapper} which is going to include all the data fields which contain data, and will ease the extraction of the data in the future. \textit{RoadRunner} is an approach which takes a reference page as a wrapper, and compares it to the rest of the pages from a single domain, and tries to build a wrapper.
	
	We have two different approaches on how to implement a \textit{RoadRunner}-like algorithm. One is based on basic string matching, without using a special data structure for recursive moving along the document and it will produce a regex-like output, and another one which uses a tree-like structure provided by \texttt{JSoup}~\cite{jsoup}.
	
	\subsection{Version 1}\label{v1}
	% @Stefan TODO
	\cite{crescenzi2001roadrunner} % stavi ovo dje ti treba
	As mentioned above, for this approach we have used 
	
	From the two input \textttt{HTML} files we remove \textttt{<head>} tag and everything in it and also we remove all \textttt{<script>s} and all comments from the code. Everything is in lowercase and we transfer them into \textttt{.xml} files using \textttt{BeautifulSoup} module. 
	
	After preprocessing we have two lists created from two input \textttt{HTMLs} (wrapper list and sample list, respectively), with elements that are tokens from the input \textttt{HTML} files. This tokens can be HTML tags with attributes or some text, and based on that we continue with matching the corresponding element of the wrapper with the corresponding element of the sample list. 
	
	
	
	\subsection{Pseudocode}
	
	
	
	
	\subsection{Output}
	Output is regex-like expression which will 
	
	
	
	
	\subsection{}
	\subsection{}
	
	
	
	
	
	
	
	
	
	
	
	
	
	\subsection{Version 2}\label{v2}
	This approach is consisted of a preprocessing step, similar to the one described in Section~\ref{v1}  in which we clear out all the attributes from all \textit{HTML} tags, apart from the links \texttt{<a></a>}, and images \texttt{<img/>}, from the input pages. We do all the preprocessing using \texttt{Jsoup}, and as a consequence we get a recursive structure of the \textit{HTML} pages. Next up is the building of the wrapper, which we further discuss in Subsection~\ref{v2-pseudo}. The core principles of this approach lie within the comparison of the nodes, and how we expand the wrapper.
	
	\subsubsection{Pseudocode} \label{v2-pseudo}
	Following the preprocessing, we do the  steps:
	\begin{enumerate}
		\item Pick a random page as a reference page, i.e. wrapper, and use the other page to compare the wrapper to.
		\item We compare the pages level by level, recursively:
		\begin{itemize}
			\item compare the current wrapper tag to the other tag (described below)
			\item if they are an exact match, we have detected static content and we add it to the wrapper
			\item if there are mismatches, we continue the comparison recursively
			\item finally, compare the leaves and mark the nodes as data nodes
		\end{itemize}
		\item Return the result as a final wrapper.
	\end{enumerate}
	
	\subsubsection{Comparison function}
	The comparison is the most important information we get to make our decision on what to do in the recursion next. As a result from it, we get a list denoting whether the nodes are:
	\begin{itemize}
		\item equal tag name, if not - continue
		\item equal number of children, if yes - it is a potential candidate for an iterator
		\item shared tag name among children, if yes - it is most likely an iterator
		\item equal text contained within them, if yes - it is static content
		\item equal link/image attributes, if not - the link/image content is dynamic
	\end{itemize}
	With the comparison function we detect the static and dynamic contents of the page, the multiple items which are equal (referred to as \textit{iterator}), and if there are heterogeneous child nodes (i.e. their tag names do not match) we handle them separately.
	
	\subsubsection{Building the wrapper}
	The task of building the wrapper is performed based on the properties obtained from the comparison function. We have a trivial case when the detected content is an exact match between the pages, in which we add the content to the wrapper, and continue with the processing. Another rather trivial case is when we need reach the leaf tags which have dynamic content and we need to generalize the node. 
	
	A more difficult case is when 
	
	\subsubsection{Node generalization}
	A node which is detected to have dynamic content, is generalized by removing all the text within it, and its children, and has generalized data such as:
	\begin{itemize}
		\item empty text is generalized to \texttt{\$data}
		\item class \texttt{optional} is added to the generalized node
		\item links are generalized to \texttt{<a href="\$link">\$link-data</a>}
		\item images are generalized to \texttt{<img src="\$img-link" alt="\$img-alt">}
	\end{itemize}
	
	\subsubsection{Output}
	The output is in the form of a generalized \textit{HTML} page which has the generalized nodes as data holders. With this output we are preserving all the static content of the page, and we mark the places where the dynamic content is.
	
	In its raw form, i.e. text, the wrapper is also in a human readable form, as the wrapper can be opened in a web browser of choice. For more clarity on the wrappers' attributes we advise to be opened in a text editor, as the output is prettified using \texttt{JSoup}, and it allows the reader a convenient experience while reading the \textit{HTML} code.
	
	\section{Conclusion}
	% @Gojko TODO
	
	
	\bibliographystyle{IEEEtran}
	\bibliography{refs}
	
\end{document}
